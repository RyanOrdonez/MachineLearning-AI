{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTSA 5504: Data Mining Pipeline\n",
    "\n",
    "## Course Overview and Quick Reference Guide\n",
    "\n",
    "This notebook serves as a comprehensive overview and quick reference guide for the key concepts, techniques, and implementations covered in this course.\n",
    "\n",
    "### Course Objectives\n",
    "- Understanding data mining pipeline components\n",
    "- Implementing data preprocessing techniques\n",
    "- Building efficient data mining workflows\n",
    "- Evaluating and optimizing pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import common libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Display settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 1: Introduction to Data Mining Pipeline\n",
    "\n",
    "### Key Concepts\n",
    "- \n",
    "\n",
    "### Important Components\n",
    "- \n",
    "\n",
    "### Code Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_basic_pipeline():\n",
    "    \"\"\"Create a basic data mining pipeline\"\"\"\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('feature_selection', SelectKBest(k=10))\n",
    "    ])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2: Data Collection and Integration\n",
    "\n",
    "### Key Concepts\n",
    "- \n",
    "\n",
    "### Important Methods\n",
    "- \n",
    "\n",
    "### Code Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_and_merge_data(file_paths, join_keys):\n",
    "    \"\"\"Load and merge multiple data sources\"\"\"\n",
    "    dataframes = [pd.read_csv(path) for path in file_paths]\n",
    "    merged_df = dataframes[0]\n",
    "    \n",
    "    for i, df in enumerate(dataframes[1:], 1):\n",
    "        merged_df = pd.merge(\n",
    "            merged_df,\n",
    "            df,\n",
    "            on=join_keys[i-1],\n",
    "            how='left'\n",
    "        )\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3: Data Preprocessing\n",
    "\n",
    "### Key Concepts\n",
    "- \n",
    "\n",
    "### Important Techniques\n",
    "- \n",
    "\n",
    "### Code Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"Comprehensive data preprocessing\"\"\"\n",
    "    # Handle missing values\n",
    "    numeric_features = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(drop='first', sparse=False))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "    \n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 4: Feature Engineering\n",
    "\n",
    "### Key Concepts\n",
    "- \n",
    "\n",
    "### Important Methods\n",
    "- \n",
    "\n",
    "### Code Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class FeatureEngineer:\n",
    "    \"\"\"Feature engineering utilities\"\"\"\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def create_date_features(self, df, date_column):\n",
    "        \"\"\"Extract date features\"\"\"\n",
    "        df[date_column] = pd.to_datetime(df[date_column])\n",
    "        df[f'{date_column}_year'] = df[date_column].dt.year\n",
    "        df[f'{date_column}_month'] = df[date_column].dt.month\n",
    "        df[f'{date_column}_day'] = df[date_column].dt.day\n",
    "        df[f'{date_column}_dayofweek'] = df[date_column].dt.dayofweek\n",
    "        return df\n",
    "    \n",
    "    def create_interaction_features(self, df, feature1, feature2, operation='multiply'):\n",
    "        \"\"\"Create interaction features\"\"\"\n",
    "        if operation == 'multiply':\n",
    "            df[f'{feature1}_{feature2}_interaction'] = df[feature1] * df[feature2]\n",
    "        elif operation == 'divide':\n",
    "            df[f'{feature1}_{feature2}_ratio'] = df[feature1] / df[feature2]\n",
    "        return df\n",
    "    \n",
    "    def create_aggregate_features(self, df, group_col, agg_col, aggs=['mean', 'std', 'min', 'max']):\n",
    "        \"\"\"Create aggregate features\"\"\"\n",
    "        agg_df = df.groupby(group_col)[agg_col].agg(aggs).reset_index()\n",
    "        return pd.merge(df, agg_df, on=group_col, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 5: Feature Selection\n",
    "\n",
    "### Key Concepts\n",
    "- \n",
    "\n",
    "### Important Methods\n",
    "- \n",
    "\n",
    "### Code Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def select_features(X, y):\n",
    "    \"\"\"Comprehensive feature selection\"\"\"\n",
    "    from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "    # Statistical selection\n",
    "    k_best = SelectKBest(score_func=f_classif, k=10)\n",
    "    X_stat = k_best.fit_transform(X, y)\n",
    "    \n",
    "    # Recursive feature elimination\n",
    "    rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=10)\n",
    "    X_rfe = rfe.fit_transform(X, y)\n",
    "    \n",
    "    # Feature importance\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X, y)\n",
    "    importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': rf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    return {\n",
    "        'statistical': X_stat,\n",
    "        'rfe': X_rfe,\n",
    "        'importance': importance\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 6: Pipeline Optimization\n",
    "\n",
    "### Key Concepts\n",
    "- \n",
    "\n",
    "### Important Techniques\n",
    "- \n",
    "\n",
    "### Code Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def optimize_pipeline(pipeline, X, y):\n",
    "    \"\"\"Optimize pipeline using grid search\"\"\"\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    \n",
    "    param_grid = {\n",
    "        'imputer__strategy': ['mean', 'median'],\n",
    "        'feature_selection__k': [5, 10, 15],\n",
    "        'classifier__n_estimators': [100, 200],\n",
    "        'classifier__max_depth': [None, 10, 20]\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "    return grid_search.best_estimator_, grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 7: Pipeline Deployment\n",
    "\n",
    "### Key Concepts\n",
    "- \n",
    "\n",
    "### Important Steps\n",
    "- \n",
    "\n",
    "### Code Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def save_pipeline(pipeline, filename):\n",
    "    \"\"\"Save pipeline to disk\"\"\"\n",
    "    import joblib\n",
    "    joblib.dump(pipeline, filename)\n",
    "    \n",
    "def load_pipeline(filename):\n",
    "    \"\"\"Load pipeline from disk\"\"\"\n",
    "    import joblib\n",
    "    return joblib.load(filename)\n",
    "\n",
    "def create_api_endpoint(pipeline):\n",
    "    \"\"\"Create FastAPI endpoint for pipeline\"\"\"\n",
    "    from fastapi import FastAPI\n",
    "    from pydantic import BaseModel\n",
    "    \n",
    "    app = FastAPI()\n",
    "    \n",
    "    class DataInput(BaseModel):\n",
    "        features: list\n",
    "        \n",
    "    @app.post(\"/predict\")\n",
    "    def predict(data: DataInput):\n",
    "        return {\"prediction\": pipeline.predict([data.features]).tolist()}\n",
    "    \n",
    "    return app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 8: Pipeline Monitoring and Maintenance\n",
    "\n",
    "### Key Concepts\n",
    "- \n",
    "\n",
    "### Important Metrics\n",
    "- \n",
    "\n",
    "### Code Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def monitor_pipeline(pipeline, X, y, monitoring_period='1d'):\n",
    "    \"\"\"Monitor pipeline performance\"\"\"\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "    import time\n",
    "    \n",
    "    metrics = {\n",
    "        'timestamp': [],\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'latency': []\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    predictions = pipeline.predict(X)\n",
    "    latency = time.time() - start_time\n",
    "    \n",
    "    metrics['timestamp'].append(pd.Timestamp.now())\n",
    "    metrics['accuracy'].append(accuracy_score(y, predictions))\n",
    "    metrics['precision'].append(precision_score(y, predictions, average='weighted'))\n",
    "    metrics['recall'].append(recall_score(y, predictions, average='weighted'))\n",
    "    metrics['latency'].append(latency)\n",
    "    \n",
    "    return pd.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources and References\n",
    "\n",
    "### Useful Libraries\n",
    "- Scikit-learn: Machine learning tools\n",
    "- Pandas: Data manipulation\n",
    "- FastAPI: API development\n",
    "- MLflow: Pipeline tracking\n",
    "\n",
    "### External Links\n",
    "- Course materials\n",
    "- Pipeline examples\n",
    "- Best practices\n",
    "\n",
    "### Personal Notes\n",
    "- Key components\n",
    "- Optimization tips\n",
    "- Maintenance checklist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
