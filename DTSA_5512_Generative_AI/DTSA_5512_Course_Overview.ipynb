{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTSA 5512: Introduction to Generative AI\n",
    "\n",
    "## Course Overview and Quick Reference Guide\n",
    "\n",
    "This notebook serves as a comprehensive overview and quick reference guide for the key concepts, techniques, and implementations covered in this course.\n",
    "\n",
    "### Course Objectives\n",
    "- Understanding generative AI principles\n",
    "- Implementing various generative models\n",
    "- Working with transformers and LLMs\n",
    "- Applying generative AI to real problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import common libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Display settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 1: Introduction to Generative AI\n",
    "\n",
    "### Key Concepts\n",
    "- \n",
    "\n",
    "### Important Terms\n",
    "- \n",
    "\n",
    "### Code Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class SimpleAutoencoder(nn.Module):\n",
    "    \"\"\"Simple autoencoder implementation\"\"\"\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2: Variational Autoencoders\n",
    "\n",
    "### Key Concepts\n",
    "- \n",
    "\n",
    "### Important Components\n",
    "- \n",
    "\n",
    "### Code Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class VAE(nn.Module):\n",
    "    \"\"\"Variational Autoencoder implementation\"\"\"\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_var = nn.Linear(128, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_var(h)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3: Generative Adversarial Networks\n",
    "\n",
    "### Key Concepts\n",
    "- \n",
    "\n",
    "### Important Components\n",
    "- \n",
    "\n",
    "### Code Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"Simple GAN Generator\"\"\"\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Simple GAN Discriminator\"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 4: Transformers and Language Models\n",
    "\n",
    "### Key Concepts\n",
    "- \n",
    "\n",
    "### Important Components\n",
    "- \n",
    "\n",
    "### Code Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_language_model(model_name=\"gpt2\"):\n",
    "    \"\"\"Load a pre-trained language model\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_text(prompt, model, tokenizer, max_length=50):\n",
    "    \"\"\"Generate text using a language model\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 5: Diffusion Models\n",
    "\n",
    "### Key Concepts\n",
    "- \n",
    "\n",
    "### Important Components\n",
    "- \n",
    "\n",
    "### Code Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class SimpleDiffusion:\n",
    "    \"\"\"Simple diffusion model implementation\"\"\"\n",
    "    def __init__(self, n_steps=1000, beta_start=1e-4, beta_end=0.02):\n",
    "        self.n_steps = n_steps\n",
    "        self.beta = torch.linspace(beta_start, beta_end, n_steps)\n",
    "        self.alpha = 1 - self.beta\n",
    "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
    "        \n",
    "    def forward_diffusion(self, x0, t):\n",
    "        \"\"\"Forward diffusion process\"\"\"\n",
    "        noise = torch.randn_like(x0)\n",
    "        alpha_t = self.alpha_bar[t]\n",
    "        mean = torch.sqrt(alpha_t) * x0\n",
    "        var = torch.sqrt(1 - alpha_t)\n",
    "        return mean + var * noise, noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 6: Advanced Architectures\n",
    "\n",
    "### Key Concepts\n",
    "- \n",
    "\n",
    "### Important Models\n",
    "- \n",
    "\n",
    "### Code Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class StyleGANBlock(nn.Module):\n",
    "    \"\"\"Simple StyleGAN-like block\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, style_dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.noise_weight = nn.Parameter(torch.zeros(1, out_channels, 1, 1))\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "        self.style_mod = nn.Linear(style_dim, out_channels * 2)\n",
    "        \n",
    "    def forward(self, x, style):\n",
    "        x = self.conv(x)\n",
    "        noise = torch.randn(x.shape[0], 1, x.shape[2], x.shape[3], device=x.device)\n",
    "        x = x + self.noise_weight * noise\n",
    "        \n",
    "        style = self.style_mod(style).unsqueeze(2).unsqueeze(3)\n",
    "        gamma, beta = style.chunk(2, dim=1)\n",
    "        x = gamma * x + beta\n",
    "        \n",
    "        return self.activation(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 7: Training and Optimization\n",
    "\n",
    "### Key Concepts\n",
    "- \n",
    "\n",
    "### Important Techniques\n",
    "- \n",
    "\n",
    "### Code Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_gan(generator, discriminator, dataloader, n_epochs=100):\n",
    "    \"\"\"Basic GAN training loop\"\"\"\n",
    "    g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002)\n",
    "    d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for real_data in dataloader:\n",
    "            batch_size = real_data.size(0)\n",
    "            real_label = torch.ones(batch_size, 1)\n",
    "            fake_label = torch.zeros(batch_size, 1)\n",
    "            \n",
    "            # Train Discriminator\n",
    "            d_optimizer.zero_grad()\n",
    "            label = ((real_label + fake_label) / 2).to(device)\n",
    "            output = discriminator(real_data.to(device))\n",
    "            d_loss_real = criterion(output, real_label.to(device))\n",
    "            \n",
    "            z = torch.randn(batch_size, generator.latent_dim).to(device)\n",
    "            fake_data = generator(z)\n",
    "            output = discriminator(fake_data.detach())\n",
    "            d_loss_fake = criterion(output, fake_label.to(device))\n",
    "            \n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "            \n",
    "            # Train Generator\n",
    "            g_optimizer.zero_grad()\n",
    "            output = discriminator(fake_data)\n",
    "            g_loss = criterion(output, real_label.to(device))\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 8: Applications and Future Directions\n",
    "\n",
    "### Key Concepts\n",
    "- \n",
    "\n",
    "### Important Applications\n",
    "- \n",
    "\n",
    "### Code Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def image_to_image_translation(source_image, style_image, model):\n",
    "    \"\"\"Simple image-to-image translation example\"\"\"\n",
    "    # Implement based on specific model architecture\n",
    "    pass\n",
    "\n",
    "def text_to_image_generation(text_prompt, model):\n",
    "    \"\"\"Simple text-to-image generation example\"\"\"\n",
    "    # Implement based on specific model architecture\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources and References\n",
    "\n",
    "### Useful Libraries\n",
    "- PyTorch: Deep learning framework\n",
    "- Transformers: NLP models\n",
    "- Diffusers: Diffusion models\n",
    "- Lightning: Training framework\n",
    "\n",
    "### External Links\n",
    "- Course materials\n",
    "- Research papers\n",
    "- Model implementations\n",
    "\n",
    "### Personal Notes\n",
    "- Key architectures\n",
    "- Training tips\n",
    "- Best practices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
